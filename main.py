import streamlit as st
import openai
import httpx
import os
import json
from pathlib import Path
from typing import List, Dict
from google import genai
from dotenv import load_dotenv, find_dotenv, set_key
from pydantic import BaseModel
import re

def extract_json_from_string(text: str) -> dict | None:
    """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»å¯èƒ½åŒ…å«å‰åç¼€æ–‡æœ¬çš„å­—ç¬¦ä¸²ä¸­æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ã€‚"""
    match = re.search(r'\{.*\}', text, re.DOTALL)
    return json.loads(match.group(0))

class Recipe(BaseModel):
    recipe_name: str
    ingredients: list[str]

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
env_file = find_dotenv()
if not env_file:
    env_file = Path(".env")
    env_file.touch()
load_dotenv(env_file)

# --- é…ç½®ç®¡ç† (ä¸åŸç‰ˆç›¸åŒ) ---
def load_config() -> dict:
    """åŠ è½½é…ç½®ï¼Œæ”¯æŒ openaiå…¼å®¹æ ¼å¼ / google åˆ†èŠ‚åµŒå¥—ç»“æ„ã€‚"""
    return {
        "provider": os.getenv("PROVIDER", "openai"),
        "proxy_url": os.getenv("PROXY_URL", ""),
        "openai": {
            "api_base": os.getenv("OPENAI_API_BASE", "https://api.mistral.ai/v1"),
            "api_key": os.getenv("OPENAI_API_KEY", ""),
            "model": os.getenv("OPENAI_MODEL", "mistral-medium-latest"),
        },
        "google": {
            "api_key": os.getenv("GOOGLE_API_KEY", ""),
            "model": os.getenv("GOOGLE_MODEL", "gemini-2.5-flash"),
        },
    }

def save_config(cfg: dict):
    """å°†é…ç½®ä¿å­˜åˆ° .env æ–‡ä»¶ã€‚"""
    set_key(env_file, "PROVIDER", cfg.get("provider", "openai"))
    set_key(env_file, "PROXY_URL", cfg.get("proxy_url", ""))
    if "openai" in cfg:
        set_key(env_file, "OPENAI_API_KEY", cfg["openai"].get("api_key", ""))
        set_key(env_file, "OPENAI_API_BASE", cfg["openai"].get("api_base", ""))
        set_key(env_file, "OPENAI_MODEL", cfg["openai"].get("model", ""))
    if "google" in cfg:
        set_key(env_file, "GOOGLE_API_KEY", cfg["google"].get("api_key", ""))
        set_key(env_file, "GOOGLE_MODEL", cfg["google"].get("model", ""))


class LLMClient:
    """ä¸€ä¸ªç»Ÿä¸€çš„ã€ç®€åŒ–çš„LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒOpenAIå…¼å®¹æ¥å£å’ŒGoogle Geminiï¼Œå¹¶ç»Ÿä¸€å¤„ç†ä»£ç†ã€‚"""
    def __init__(self, config: dict):
        self.full_config = config
        self.provider = config.get("provider", "openai")
        proxy_url = config.get("proxy_url")
        provider_cfg = config.get(self.provider)
        self.model = provider_cfg.get("model")
        api_key = provider_cfg.get("api_key")

        if self.provider == "google":
            if proxy_url:
                os.environ["HTTP_PROXY"] = proxy_url
                os.environ["HTTPS_PROXY"] = proxy_url
            self.client = genai.Client(api_key=api_key)
        else:  # openai å…¼å®¹
            http_client = httpx.Client(proxy=proxy_url or None)
            self.client = openai.OpenAI(
                api_key=api_key,
                base_url=provider_cfg.get("api_base", ""),
                http_client=http_client,
            )

    def call(self, messages: List[Dict], json_mode: bool = False) -> str:
        """æ ¹æ®æä¾›å•†è°ƒç”¨ç›¸åº”çš„LLM API"""
        if self.provider == "google":
            config = {
                "response_mime_type": "application/json",
                "response_schema": Recipe,
            } if json_mode else {}
            
            response = self.client.models.generate_content(
                model=self.model, 
                config=config,
                contents=messages[0]["content"],
            )
            return response.text
        else: # openai å…¼å®¹
            extra_params = {"response_format": {"type": "json_object"}} if json_mode else {}
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                **extra_params,
            )
            return response.choices[0].message.content


# --- Prompt æ¨¡æ¿ ---
ROLE_INSTRUCTION = "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä¸“åˆ©ä»£ç†å¸ˆï¼Œæ“…é•¿æ’°å†™ç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥è°¨çš„ä¸“åˆ©ç”³è¯·æ–‡ä»¶ã€‚"

# 0. åˆ†æä»£ç†
PROMPT_ANALYZE = (
    f"{ROLE_INSTRUCTION}\n"
    "ä»»åŠ¡æè¿°ï¼šè¯·ä»”ç»†é˜…è¯»å¹¶åˆ†æä»¥ä¸‹æŠ€æœ¯äº¤åº•ææ–™ï¼Œå°†å…¶æ‹†è§£å¹¶æç‚¼æˆä¸€ä¸ªç»“æ„åŒ–çš„JSONå¯¹è±¡ã€‚è¯·ç¡®ä¿JSONæ ¼å¼æ­£ç¡®æ— è¯¯ã€‚\n"
    "JSONç»“æ„åº”åŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n"
    "1. `problem_statement`: è¯¥å‘æ˜æ—¨åœ¨è§£å†³çš„ç°æœ‰æŠ€æœ¯ä¸­çš„å…·ä½“é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆ1-2å¥è¯ï¼‰\n"
    "2. `core_inventive_concept`: æœ¬å‘æ˜çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æˆ–æœ€å…³é”®çš„æŠ€æœ¯ç‰¹å¾æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆåŒºåˆ«äºç°æœ‰æŠ€æœ¯çš„æœ¬è´¨ï¼‰\n"
    "3. `technical_solution_summary`: ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬å‘æ˜æå‡ºçš„æŠ€æœ¯æ–¹æ¡ˆæ¦‚è¿°ï¼ŒåŒ…æ‹¬å…³é”®ç»„ä»¶æˆ–æ­¥éª¤ã€‚\n"
    "4. `key_components_or_steps`: åˆ—å‡ºå®ç°è¯¥æ–¹æ¡ˆæ‰€éœ€çš„ä¸»è¦ç‰©ç†ç»„ä»¶æˆ–å·¥è‰ºæ­¥éª¤çš„æ¸…å•ï¼ˆä½¿ç”¨åˆ—è¡¨æ ¼å¼ï¼‰ã€‚\n"
    "5. `achieved_effects`: ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œæœ¬å‘æ˜èƒ½å¸¦æ¥çš„å…·ä½“ã€å¯é‡åŒ–çš„æœ‰ç›Šæ•ˆæœæ˜¯ä»€ä¹ˆï¼Ÿ\n\n"
    "æŠ€æœ¯äº¤åº•ææ–™ï¼š\n{user_input}"
)

# 1. å‘æ˜åç§°ä»£ç†
PROMPT_TITLE = (
    f"{ROLE_INSTRUCTION}\n"
    "ä»»åŠ¡æè¿°ï¼šè¯·æ ¹æ®ä»¥ä¸‹æ ¸å¿ƒåˆ›æ–°ç‚¹å’ŒæŠ€æœ¯æ–¹æ¡ˆï¼Œæç‚¼å‡ºç¬¦åˆä¸­å›½ä¸“åˆ©å‘½åè§„èŒƒä¸”ä¸è¶…è¿‡25ä¸ªæ±‰å­—çš„å‘æ˜åç§°ã€‚è¦æ±‚ç®€æ´æ˜äº†å¹¶å‡†ç¡®åæ˜ æŠ€æœ¯å†…å®¹ã€‚è¯·åªè¿”å›å‘æ˜åç§°æœ¬èº«ï¼Œä¸è¦æ·»åŠ ä»»ä½•å¤šä½™å†…å®¹ã€‚\n\n"
    "æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š{core_inventive_concept}\n"
    "æŠ€æœ¯æ–¹æ¡ˆæ¦‚è¿°ï¼š{technical_solution_summary}"
)

# 2. èƒŒæ™¯æŠ€æœ¯ä»£ç†
PROMPT_BACKGROUND = (
    f"{ROLE_INSTRUCTION}\n"
    "ä»»åŠ¡æè¿°ï¼šè¯·å›´ç»•ä»¥ä¸‹â€œå¾…è§£å†³çš„æŠ€æœ¯é—®é¢˜â€ï¼Œæ’°å†™â€œäºŒã€ç°æœ‰æŠ€æœ¯ï¼ˆèƒŒæ™¯æŠ€æœ¯ï¼‰â€ç« èŠ‚ã€‚ä½ éœ€è¦åˆ†æä¸æ­¤é—®é¢˜æœ€ç›¸å…³çš„ç°æœ‰æŠ€æœ¯çŠ¶å†µï¼Œå¹¶å®äº‹æ±‚æ˜¯åœ°æŒ‡å‡ºç°æœ‰æŠ€æœ¯å­˜åœ¨çš„é—®é¢˜åŠåŸå› ã€‚è¯·ä½¿ç”¨ä¸“ä¸šã€å®¢è§‚çš„ä¸­æ–‡ï¼Œå¹¶ä»¥ Markdown æ ¼å¼è¿”å›ã€‚\n\n"
    "å‘æ˜åç§°ï¼š{title}\n"
    "å¾…è§£å†³çš„æŠ€æœ¯é—®é¢˜ï¼š{problem_statement}"
)

# 3. å‘æ˜å†…å®¹ä»£ç†
PROMPT_INVENTION_CONTENT = (
    f"{ROLE_INSTRUCTION}\n"
    "ä»»åŠ¡æè¿°ï¼šè¯·ä¾æ®ä»¥ä¸‹ç»“æ„åŒ–çš„å‘æ˜ä¿¡æ¯å’Œå·²ç”Ÿæˆçš„èƒŒæ™¯æŠ€æœ¯ï¼Œæ’°å†™â€œä¸‰ã€å‘æ˜å†…å®¹â€ç« èŠ‚ã€‚å†…å®¹éœ€ä¸¥æ ¼å¯¹åº”ï¼Œé€»è¾‘æ¸…æ™°ã€‚\n"
    "1. å‘æ˜ç›®çš„ï¼šæ¸…æ™°å›åº”èƒŒæ™¯æŠ€æœ¯ä¸­æå‡ºçš„é—®é¢˜ï¼ˆæºè‡ª`problem_statement`ï¼‰ã€‚\n"
    "2. æŠ€æœ¯è§£å†³æ–¹æ¡ˆï¼šåŸºäº`technical_solution_summary`å’Œ`key_components_or_steps`å±•å¼€ï¼Œæ¸…æ¥šã€å®Œæ•´åœ°æè¿°æœ¬å‘æ˜çš„æ–¹æ¡ˆã€‚\n"
    "3. æŠ€æœ¯æ•ˆæœï¼šå…·ä½“ã€å®äº‹æ±‚æ˜¯åœ°æè¿°æœ¬å‘æ˜å¯è¾¾åˆ°çš„æ•ˆæœï¼ˆæºè‡ª`achieved_effects`ï¼‰ã€‚\n"
    "è¯·ä½¿ç”¨ä¸“ä¸šã€ä¸¥è°¨çš„ä¸­æ–‡ï¼Œå¹¶ä»¥ Markdown æ ¼å¼è¿”å›ã€‚\n\n"
    "å‘æ˜åç§°ï¼š{title}\n"
    "èƒŒæ™¯æŠ€æœ¯ç« èŠ‚ï¼ˆä¾›å‚è€ƒï¼‰ï¼š\n{background}\n\n"
    "ç»“æ„åŒ–å‘æ˜ä¿¡æ¯ï¼š\n"
    "  - å¾…è§£å†³é—®é¢˜: {problem_statement}\n"
    "  - æŠ€æœ¯æ–¹æ¡ˆæ¦‚è¿°: {technical_solution_summary}\n"
    "  - å…³é”®ç»„ä»¶/æ­¥éª¤: {key_components_or_steps}\n"
    "  - æœ‰ç›Šæ•ˆæœ: {achieved_effects}"
)

# 4. å…·ä½“å®æ–½æ–¹å¼ä»£ç†
PROMPT_IMPLEMENTATION = (
    f"{ROLE_INSTRUCTION}\n"
    "ä»»åŠ¡æè¿°ï¼šè¯·å°†â€œå‘æ˜å†…å®¹â€ä¸­é˜è¿°çš„æŠ€æœ¯æ–¹æ¡ˆå…·ä½“åŒ–ï¼Œæ’°å†™â€œå››ã€å…·ä½“å®æ–½æ–¹å¼â€ç« èŠ‚ã€‚è¯·è‡³å°‘ä¸¾å‡ºä¸€ä¸ªæ˜ç¡®ã€å¯æ“ä½œçš„å…·ä½“å®æ–½ä¾‹ï¼Œå¯ç»“åˆâ€œå…³é”®ç»„ä»¶/æ­¥éª¤æ¸…å•â€è¿›è¡Œè¯¦ç»†è¯´æ˜ã€‚è¯·ä»¥ Markdown æ ¼å¼è¿”å›ã€‚\n\n"
    "å‘æ˜åç§°ï¼š{title}\n"
    "å‘æ˜å†…å®¹ç« èŠ‚ï¼ˆä½œä¸ºè“æœ¬ï¼‰ï¼š\n{invention_content}\n\n"
    "å…³é”®ç»„ä»¶/æ­¥éª¤æ¸…å•ï¼ˆä¾›å‚è€ƒï¼‰ï¼š\n{key_components_or_steps}"
)

# --- Streamlit åº”ç”¨ç•Œé¢ ---
def render_sidebar(config: dict):
    """æ¸²æŸ“ä¾§è¾¹æ å¹¶è¿”å›æ›´æ–°åçš„é…ç½®å­—å…¸ã€‚"""
    with st.sidebar:
        st.header("âš™ï¸ API é…ç½®")
        provider_map = {"OpenAIå…¼å®¹": "openai", "Google": "google"}
        provider_keys = list(provider_map.keys())
        current_provider_key = next((key for key, value in provider_map.items() if value == config.get("provider")), "OpenAIå…¼å®¹")
        
        selected_provider_display = st.radio(
            "æ¨¡å‹æä¾›å•†", options=provider_keys, 
            index=provider_keys.index(current_provider_key),
            horizontal=True
        )
        config["provider"] = provider_map[selected_provider_display]

        p_cfg = config[config["provider"]]
        p_cfg["api_key"] = st.text_input("API Key", value=p_cfg.get("api_key", ""), type="password", key=f"{config['provider']}_api_key")

        if config["provider"] == "openai":
            p_cfg["api_base"] = st.text_input("API åŸºç¡€åœ°å€", value=p_cfg.get("api_base", ""), key="openai_api_base")
            p_cfg["model"] = st.text_input("æ¨¡å‹åç§°", value=p_cfg.get("model", ""), key="openai_model")
        else:
            p_cfg["model"] = st.text_input("æ¨¡å‹åç§°", value=p_cfg.get("model", ""), key="google_model")

        config["proxy_url"] = st.text_input(
            "ä»£ç† URL (å¯é€‰)", value=config.get("proxy_url", ""), 
            placeholder="http://127.0.0.1:7890"
        )

        if st.button("ä¿å­˜é…ç½®"):
            save_config(config)
            st.success("é…ç½®å·²ä¿å­˜ï¼")
            if 'llm_client' in st.session_state:
                del st.session_state.llm_client
            st.rerun()

def initialize_session_state():
    """åˆå§‹åŒ–æ‰€æœ‰éœ€è¦çš„ä¼šè¯çŠ¶æ€å˜é‡ã€‚"""
    if "stage" not in st.session_state:
        st.session_state.stage = "input"
    if "config" not in st.session_state:
        st.session_state.config = load_config()
    if "user_input" not in st.session_state:
        st.session_state.user_input = ""
    if "structured_brief" not in st.session_state:
        st.session_state.structured_brief = {}
    for key in ["title", "background", "invention", "implementation"]:
        if key not in st.session_state:
            st.session_state[key] = ""

def main():
    st.set_page_config(page_title="ä¸“åˆ©æ’°å†™åŠ©æ‰‹", layout="wide", page_icon="ğŸ“")
    st.title("ğŸ“ æ™ºèƒ½ä¸“åˆ©ç”³è¯·ä¹¦æ’°å†™åŠ©æ‰‹")

    initialize_session_state()
    render_sidebar(st.session_state.config)

    active_provider = st.session_state.config["provider"]
    if not st.session_state.config.get(active_provider, {}).get("api_key"):
        st.warning("è¯·åœ¨å·¦ä¾§è¾¹æ é…ç½®å¹¶ä¿å­˜æ‚¨çš„ API Keyã€‚")
        st.stop()

    # ç¼“å­˜LLMå®¢æˆ·ç«¯å®ä¾‹
    if 'llm_client' not in st.session_state:
        st.session_state.llm_client = LLMClient(st.session_state.config)
    llm_client = st.session_state.llm_client


    # --- é˜¶æ®µä¸€ï¼šè¾“å…¥æ ¸å¿ƒæ„æ€ ---
    st.header("Step 1ï¸âƒ£: è¾“å…¥æ ¸å¿ƒæŠ€æœ¯æ„æ€")
    user_input = st.text_area(
        "åœ¨æ­¤å¤„ç²˜è´´æ‚¨çš„æŠ€æœ¯äº¤åº•ã€é¡¹ç›®ä»‹ç»ã€æˆ–ä»»ä½•æè¿°å‘æ˜çš„æ–‡å­—ï¼š", 
        value=st.session_state.user_input,
        height=250, 
        key="user_input_area"
    )
    if st.button("ğŸ”¬ åˆ†æå¹¶æç‚¼æ ¸å¿ƒè¦ç´ ", type="primary", disabled=(st.session_state.stage != "input")):
        if user_input:
            st.session_state.user_input = user_input
            prompt = PROMPT_ANALYZE.format(user_input=user_input)
            with st.spinner("æ­£åœ¨è°ƒç”¨åˆ†æä»£ç†ï¼Œè¯·ç¨å€™..."):
                try:
                    is_json_mode = st.session_state.config["provider"] == "openai"
                    response_str = llm_client.call([{"role": "user", "content": prompt}], json_mode=is_json_mode)
                    st.session_state.structured_brief = extract_json_from_string(response_str)
                    st.session_state.stage = "review_brief"
                    st.rerun()
                except (json.JSONDecodeError, KeyError) as e:
                    st.error(f"æ— æ³•è§£ææ¨¡å‹è¿”å›çš„æ ¸å¿ƒè¦ç´ ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è¾“å‡ºæˆ–å°è¯•è°ƒæ•´è¾“å…¥ã€‚é”™è¯¯: {e}\næ¨¡å‹åŸå§‹è¿”å›: \n{response_str}")
        else:
            st.warning("è¯·è¾“å…¥æ‚¨çš„æŠ€æœ¯æ„æ€ã€‚")

    # --- é˜¶æ®µäºŒï¼šå®¡æ ¸å¹¶ç¡®è®¤æ ¸å¿ƒè¦ç´  ---
    if st.session_state.stage == "review_brief":
        st.header("Step 2ï¸âƒ£: å®¡æ ¸å¹¶ç¡®è®¤æ ¸å¿ƒè¦ç´ ")
        st.info("è¯·æ£€æŸ¥å¹¶å¯ç¼–è¾‘ç”±AIæç‚¼çš„å‘æ˜æ ¸å¿ƒä¿¡æ¯ï¼Œè¿™å°†ä½œä¸ºåç»­æ‰€æœ‰ç« èŠ‚æ’°å†™çš„åŸºç¡€ã€‚")
        
        brief = st.session_state.structured_brief
        brief['problem_statement'] = st.text_area("1. å¾…è§£å†³çš„æŠ€æœ¯é—®é¢˜", value=brief.get('problem_statement', ''), height=100)
        brief['core_inventive_concept'] = st.text_area("2. æ ¸å¿ƒåˆ›æ–°ç‚¹", value=brief.get('core_inventive_concept', ''), height=100)
        brief['technical_solution_summary'] = st.text_area("3. æŠ€æœ¯æ–¹æ¡ˆæ¦‚è¿°", value=brief.get('technical_solution_summary', ''), height=100)
        
        key_steps_list = brief.get('key_components_or_steps', [])
        key_steps_str = "\n".join(key_steps_list) if isinstance(key_steps_list, list) else key_steps_list
        edited_steps_str = st.text_area("4. å…³é”®ç»„ä»¶/æ­¥éª¤æ¸…å•", value=key_steps_str, height=100)
        brief['key_components_or_steps'] = [line.strip() for line in edited_steps_str.split('\n') if line.strip()]
        brief['achieved_effects'] = st.text_area("5. æœ‰ç›Šæ•ˆæœ", value=brief.get('achieved_effects', ''), height=100)

        col1, col2 = st.columns(2)
        if col1.button("âœ… ç¡®è®¤æ ¸å¿ƒè¦ç´ ï¼Œå¼€å§‹æ’°å†™", type="primary"):
            st.session_state.structured_brief = brief
            st.session_state.stage = "writing"
            st.rerun()
        if col2.button("è¿”å›é‡æ–°è¾“å…¥"):
            st.session_state.stage = "input"
            st.rerun()

    # --- é˜¶æ®µä¸‰ï¼šåˆ†æ­¥ç”Ÿæˆä¸æ’°å†™ ---
    if st.session_state.stage == "writing":
        st.header("Step 3ï¸âƒ£: é€ç« ç”Ÿæˆä¸ç¼–è¾‘ä¸“åˆ©è‰ç¨¿")
        brief = st.session_state.structured_brief

        # åŠ¨æ€ç”Ÿæˆå„ä¸ªéƒ¨åˆ†
        sections = {
            "title": ("å‘æ˜åç§°", PROMPT_TITLE, {"core_inventive_concept": brief['core_inventive_concept'], "technical_solution_summary": brief['technical_solution_summary']}),
            "background": ("èƒŒæ™¯æŠ€æœ¯", PROMPT_BACKGROUND, {"title": st.session_state.title, "problem_statement": brief['problem_statement']}),
            "invention": ("å‘æ˜å†…å®¹", PROMPT_INVENTION_CONTENT, {"title": st.session_state.title, "background": st.session_state.background, **brief}),
            "implementation": ("å…·ä½“å®æ–½æ–¹å¼", PROMPT_IMPLEMENTATION, {"title": st.session_state.title, "invention_content": st.session_state.invention, "key_components_or_steps": "\n".join(brief['key_components_or_steps'])})
        }

        # æŒ‰ç…§é¡ºåºæ£€æŸ¥å¹¶ç”Ÿæˆ
        for key, (label, prompt_template, format_args) in sections.items():
            with st.expander(f"**{label}**", expanded=not st.session_state[key]):
                if not st.session_state[key]:
                    # åªæœ‰å‰ç½®æ¡ä»¶æ»¡è¶³æ—¶ï¼Œæ‰æ˜¾ç¤ºç”ŸæˆæŒ‰é’®
                    if all(val for k, val in format_args.items() if k in st.session_state and isinstance(st.session_state[k], str)):
                        if st.button(f"âœï¸ ç”Ÿæˆ{label}", key=f"btn_{key}"):
                            with st.spinner(f"æ­£åœ¨è°ƒç”¨{label}ä»£ç†..."):
                                prompt = prompt_template.format(**format_args)
                                response = llm_client.call([{"role": "user", "content": prompt}])
                                st.session_state[key] = response.strip()
                                st.rerun()
                    else:
                        st.info(f"è¯·å…ˆç”Ÿæˆå‰ç½®ç« èŠ‚ï¼ˆå¦‚ï¼šå‘æ˜åç§°ï¼‰ä»¥ç»§ç»­ã€‚")
                
                # æ˜¾ç¤ºå·²ç”Ÿæˆçš„å†…å®¹ä¾›ç¼–è¾‘
                if st.session_state[key]:
                    if key == 'title':
                        st.session_state[key] = st.text_input(label, value=st.session_state[key], key=f"edit_{key}")
                    else:
                        st.session_state[key] = st.text_area(label, value=st.session_state[key], height=300, key=f"edit_{key}")

        # --- é˜¶æ®µå››ï¼šé¢„è§ˆä¸ä¸‹è½½ ---
        if all(st.session_state[key] for key in sections):
            st.header("Step 4ï¸âƒ£: é¢„è§ˆä¸ä¸‹è½½")
            st.markdown("---")
            full_text = (
                f"# ä¸€ã€å‘æ˜åç§°\n{st.session_state.title}\n\n"
                f"{st.session_state.background}\n\n"
                f"{st.session_state.invention}\n\n"
                f"{st.session_state.implementation}"
            )
            st.subheader("å®Œæ•´è‰ç¨¿é¢„è§ˆ")
            st.markdown(full_text)
            st.download_button("ğŸ“„ ä¸‹è½½å®Œæ•´è‰ç¨¿ (.md)", full_text, file_name=f"{st.session_state.title}_patent_draft.md")

if __name__ == "__main__":
    main()